{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set the path for the train and test data directories\n",
    "train_dir = '/path/to/fer2013/train'\n",
    "test_dir = '/path/to/fer2013/test'\n",
    "\n",
    "# Set the batch size and number of epochs\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "# Define the input shape of the images\n",
    "input_shape = (48, 48, 1)\n",
    "\n",
    "# Define the model architecture\n",
    "input_layer = Input(shape=input_shape)\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation='relu')(input_layer)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(128, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(7, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model with categorical crossentropy loss and Adam optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define data generators for training and testing\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(48, 48),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Set up early stopping callback to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model on the train data and evaluate it on the test data\n",
    "model.fit(train_generator,\n",
    "          epochs=epochs,\n",
    "          validation_data=test_generator,\n",
    "          callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('fer2013.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, y_train, X_test, y_test = [], [], [], []\n",
    "for i in range(len(data)):\n",
    "    emotion, pixels, usage = data.iloc[i,:].values\n",
    "    img = np.array([int(x) for x in pixels.split()]).reshape(48,48)\n",
    "    if usage == 'Training':\n",
    "        X_train.append(cv2.resize(img.astype('uint8'), (width, height)))\n",
    "        y_train.append(emotion)\n",
    "    elif usage == 'PublicTest':\n",
    "        X_test.append(cv2.resize(img.astype('uint8'), (width, height)))\n",
    "        y_test.append(emotion)\n",
    "\n",
    "# Convert to numpy arrays and normalize pixel values\n",
    "X_train = np.array(X_train).astype('float32') / 255.0\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test).astype('float32') / 255.0\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Convert labels to categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from skimage.filters import gabor_kernel\n",
    "\n",
    "# Define function to extract LBP features\n",
    "def extract_lbp_features(image):\n",
    "    # define the LBP parameters\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    method = 'uniform'\n",
    "    \n",
    "    # compute the LBP features for the image\n",
    "    lbp = local_binary_pattern(image, n_points, radius, method)\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-7)\n",
    "    \n",
    "    # return the features as a 1D NumPy array\n",
    "    return hist.ravel()\n",
    "\n",
    "# Define function to extract HOG features\n",
    "def extract_hog_features(image):\n",
    "    # define the HOG parameters\n",
    "    orientations = 9\n",
    "    pixels_per_cell = (8, 8)\n",
    "    cells_per_block = (2, 2)\n",
    "    visualize = False\n",
    "    \n",
    "    # compute the HOG features for the image\n",
    "    hog_features = hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell,\n",
    "                       cells_per_block=cells_per_block, visualize=visualize)\n",
    "    \n",
    "    # return the features as a 1D NumPy array\n",
    "    return hog_features.ravel()\n",
    "\n",
    "# Define function to extract Gabor features\n",
    "def extract_gabor_features(image):\n",
    "    # define the Gabor parameters\n",
    "    freqs = [0.05, 0.1, 0.2]\n",
    "    thetas = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    kernels = []\n",
    "    \n",
    "    # create the Gabor kernels\n",
    "    for freq in freqs:\n",
    "        for theta in thetas:\n",
    "            kernel = np.real(gabor_kernel(freq, theta=theta))\n",
    "            kernels.append(kernel)\n",
    "            \n",
    "    # compute the Gabor features for the image\n",
    "    gabor_features = np.zeros((len(kernels), 2), dtype=np.double)\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        filtered = ndi.convolve(image, kernel, mode='wrap')\n",
    "        gabor_features[k, 0] = filtered.mean()\n",
    "        gabor_features[k, 1] = filtered.var()\n",
    "    \n",
    "    # return the features as a 1D NumPy array\n",
    "    return gabor_features.ravel()\n",
    "\n",
    "# Define the model architecture\n",
    "input_shape = (width, height, 1)\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Convolutional layers for image data\n",
    "x = Conv2D(32, kernel_size=(3,3), activation='relu')(input_layer)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(64, kernel_size=(3,3), activation='relu')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(128, kernel_size=(3,3), activation='relu')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Fully connected layers for LBP, HOG, and Gabor features\n",
    "lbp_input = Input(shape=(26,), name='lbp_input')\n",
    "hog_input = Input(shape=(3780,), name='hog_input')\n",
    "gabor_input = Input(shape=(15,), name='gabor_input')\n",
    "\n",
    "\n",
    "# Concatenate the convolutional and fully connected layers\n",
    "merged = tf.keras.layers.concatenate([x, lbp_dense, hog_dense, gabor_dense])\n",
    "\n",
    "# Add additional fully connected layers\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "merged = Dense(128, activation='relu')(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "output_layer = Dense(num_classes, activation='softmax')(merged)\n",
    "\n",
    "# Define the complete model\n",
    "model = Model(inputs=[input_layer, lbp_input, hog_input, gabor_input], outputs=output_layer)\n",
    "\n",
    "# Define the optimizer, loss function, and metrics\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "loss_function = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics)\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(patience=5)\n",
    "\n",
    "# Define the data generators for training and validation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=10, width_shift_range=0.1,\n",
    "height_shift_range=0.1, shear_range=0.1, zoom_range=0.1,\n",
    "horizontal_flip=True, vertical_flip=False, fill_mode='nearest',\n",
    "preprocessing_function=extract_lbp_features)\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=extract_lbp_features)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(width, height),\n",
    "color_mode='grayscale', batch_size=batch_size,\n",
    "class_mode='categorical', shuffle=True)\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(val_dir, target_size=(width, height),\n",
    "color_mode='grayscale', batch_size=batch_size,\n",
    "class_mode='categorical', shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=num_epochs, validation_data=val_generator,\n",
    "callbacks=[early_stopping], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Whole code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage import io, color, transform, feature, filters\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "# Define function to extract LBP features\n",
    "def extract_lbp_features(image):\n",
    "    # define the LBP parameters\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    method = 'uniform'\n",
    "    \n",
    "    # compute the LBP features for the image\n",
    "    lbp = local_binary_pattern(image, n_points, radius, method)\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-7)\n",
    "    \n",
    "    # return the features as a 1D NumPy array\n",
    "    return hist.ravel()\n",
    "\n",
    "# Define function to extract HOG features\n",
    "def extract_hog_features(image):\n",
    "    # define the HOG parameters\n",
    "    orientations = 9\n",
    "    pixels_per_cell = (8, 8)\n",
    "    cells_per_block = (2, 2)\n",
    "    visualize = False\n",
    "    \n",
    "    # compute the HOG features for the image\n",
    "    hog_features = hog(image, orientations=orientations, pixels_per_cell=pixels_per_cell,\n",
    "                       cells_per_block=cells_per_block, visualize=visualize)\n",
    "    \n",
    "    # return the features as a 1D NumPy array\n",
    "    return hog_features.ravel()\n",
    "\n",
    "# Define function to extract Gabor features\n",
    "def extract_gabor_features(image):\n",
    "    # define the Gabor parameters\n",
    "    freqs = [0.05, 0.1, 0.2]\n",
    "    thetas = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    kernels = []\n",
    "    \n",
    "    # create the Gabor kernels\n",
    "    for freq in freqs:\n",
    "        for theta in thetas:\n",
    "            kernel = np.real(gabor_kernel(freq, theta=theta))\n",
    "            kernels.append(kernel)\n",
    "            \n",
    "    # compute the Gabor features for the image\n",
    "    gabor_features = np.zeros((len(kernels), 2), dtype=np.double)\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        filtered = ndi.convolve(image, kernel, mode='wrap')\n",
    "        gabor_features[k, 0] = filtered.mean()\n",
    "        gabor_features[k, 1] = filtered.var()\n",
    "    \n",
    "    # return the features as a 1D NumPy array\n",
    "    return gabor_features.ravel()\n",
    "\n",
    "# Define function to load images and extract features\n",
    "def load_images(folder):\n",
    "    image_filenames = os.listdir(folder)\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    \n",
    "    for filename in image_filenames:\n",
    "        label = int(filename.split('_')[0])\n",
    "        image = io.imread(os.path.join(folder, filename))\n",
    "        image = transform.resize(image, (48, 48))\n",
    "        gray_image = color.rgb2gray(image)\n",
    "        \n",
    "        # extract features from the image\n",
    "        lbp_features = extract_lbp_features(gray_image)\n",
    "        hog_features = extract_hog_features(gray_image)\n",
    "        gabor_features = extract_gabor_features(gray_image)\n",
    "        \n",
    "        # concatenate the features and original image data\n",
    "        features = np.concatenate((lbp_features, hog_features, gabor_features))\n",
    "        \n",
    "        # add to the lists of images and labels\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return np.array(images), np.array(labels), np.array(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training and testing data\n",
    "train_images, train_labels = load_images(train_dir)\n",
    "test_images, test_labels = load_images(test_dir)\n",
    "\n",
    "#Reshape data for use in the neural network\n",
    "train_images = train_images.reshape(-1, feature_size)\n",
    "test_images = test_images.reshape(-1, feature_size)\n",
    "\n",
    "#Normalize pixel values to be between 0 and 1\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "#Convert labels to categorical variables\n",
    "train_labels = to_categorical(train_labels, num_classes=7)\n",
    "test_labels = to_categorical(test_labels, num_classes=7)\n",
    "\n",
    "#Define the neural network architecture\n",
    "input_layer = Input(shape=(feature_size,))\n",
    "x = Dense(128, activation='relu')(input_layer)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "output_layer = Dense(7, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Train the model\n",
    "model.fit(train_images, train_labels, epochs=20, batch_size=64, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, concatenate\n",
    "\n",
    "# Define inputs for image and feature data\n",
    "input_image = Input(shape=(48, 48, 1))\n",
    "input_features = Input(shape=(lbp_features.shape[0] + hog_features.shape[0] + gabor_features.shape[0],))\n",
    "\n",
    "# Convolutional layers for image data\n",
    "x = Conv2D(32, kernel_size=(3,3), activation='relu')(input_image)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(64, kernel_size=(3,3), activation='relu')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(128, kernel_size=(3,3), activation='relu')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Fully connected layers for feature data\n",
    "y = Dense(256, activation='relu')(input_features)\n",
    "y = Dropout(0.5)(y)\n",
    "y = Dense(128, activation='relu')(y)\n",
    "y = Dropout(0.5)(y)\n",
    "y = Dense(64, activation='relu')(y)\n",
    "\n",
    "# Concatenate image and feature data and add more fully connected layers\n",
    "combined = concatenate([x, y])\n",
    "z = Dense(64, activation='relu')(combined)\n",
    "z = Dropout(0.5)(z)\n",
    "z = Dense(32, activation='relu')(z)\n",
    "output = Dense(num_classes, activation='softmax')(z)\n",
    "\n",
    "# Define model with inputs and outputs\n",
    "model = Model(inputs=[input_image, input_features], outputs=output)\n",
    "\n",
    "# Compile model with loss function and optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskimage\u001b[39;00m \u001b[39mimport\u001b[39;00m io, color, transform\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskimage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature\u001b[39;00m \u001b[39mimport\u001b[39;00m local_binary_pattern, hog, greycomatrix, greycoprops\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage import io, color, transform\n",
    "from skimage.feature import local_binary_pattern, hog, greycomatrix, greycoprops\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dense, Dropout, Flatten, concatenate\n",
    "\n",
    "# Define functions to extract LBP, HOG, and Gabor features\n",
    "def extract_lbp_features(image):\n",
    "    lbp_radius = 3\n",
    "    lbp_n_points = 8 * lbp_radius\n",
    "    lbp_method = 'uniform'\n",
    "    lbp_histogram_bins = 26\n",
    "    \n",
    "    lbp = local_binary_pattern(image, lbp_n_points, lbp_radius, method=lbp_method)\n",
    "    lbp_histogram, _ = np.histogram(lbp, bins=lbp_histogram_bins, range=(0, lbp_histogram_bins), density=True)\n",
    "    \n",
    "    return lbp_histogram\n",
    "\n",
    "def extract_hog_features(image):\n",
    "    hog_orientations = 8\n",
    "    hog_pixels_per_cell = (6, 6)\n",
    "    hog_cells_per_block = (2, 2)\n",
    "    \n",
    "    hog_features = hog(image, orientations=hog_orientations, pixels_per_cell=hog_pixels_per_cell, cells_per_block=hog_cells_per_block)\n",
    "    \n",
    "    return hog_features\n",
    "\n",
    "def extract_gabor_features(image):\n",
    "    gabor_frequencies = [0.05, 0.25]\n",
    "    gabor_angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    \n",
    "    gabor_features = []\n",
    "    for frequency in gabor_frequencies:\n",
    "        for angle in gabor_angles:\n",
    "            real, imag = greycomatrix(image, [1], [angle], levels=256, symmetric=True, normed=True)\n",
    "            gabor_response = np.abs(np.fft.fftshift(np.fft.fft2(real * np.cos(angle) + imag * np.sin(angle))))\n",
    "            gabor_filtered = gabor_response * np.exp(-0.5 * ((gabor_response/gabor_response.mean())**2))\n",
    "            gabor_features.append(gabor_filtered.mean())\n",
    "    \n",
    "    return gabor_features\n",
    "\n",
    "# Define function to load images and extract features\n",
    "def load_images(folder):\n",
    "    image_filenames = os.listdir(folder)\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in image_filenames:\n",
    "        label = int(filename.split('_')[0])\n",
    "        image = io.imread(os.path.join(folder, filename))\n",
    "        image = transform.resize(image, (48, 48))\n",
    "        gray_image = color.rgb2gray(image)\n",
    "        \n",
    "        # extract features from the image\n",
    "        lbp_features = extract_lbp_features(gray_image)\n",
    "        hog_features = extract_hog_features(gray_image)\n",
    "        gabor_features = extract_gabor_features(gray_image)\n",
    "        \n",
    "        # concatenate the features and original image data\n",
    "        features = np.concatenate((lbp_features, hog_features, gabor_features))\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "        \n",
    "    return np.array(images), np.array(features), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and testing data\n",
    "train_folder = 'C:/Users/HP/Desktop/face_emotion_github/Data/Fer2013/archive/train/'\n",
    "test_folder = 'C:/Users/HP/Desktop/face_emotion_github/Data/Fer2013/archive/test/'\n",
    "train_images, train_features, train_labels = load_images()\n",
    "test_images, test_features, test_labels = load_images(test_folder)\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_layer = Input(shape=(48, 48, 3))\n",
    "x = Conv2D(32, kernel_size=(3,3), activation='relu')(input_layer)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(64, kernel_size=(3,3), activation='relu')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(128, kernel_size=(3,3), activation='relu')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "lbp_input = Input(shape=(26,))\n",
    "hog_input = Input(shape=(324,))\n",
    "gabor_input = Input(shape=(40,))\n",
    "# concatenated = Concatenate()([x, lbp_input, hog_input, gabor_input])\n",
    "# dense1 = Dense(256, activation='relu')(concatenated)\n",
    "# dropout1 = Dropout(0.5)(dense1)\n",
    "# dense2 = Dense(128, activation='relu')(dropout1)\n",
    "# dropout2 = Dropout(0.5)(dense2)\n",
    "# output_layer = Dense(num_classes, activation='softmax')(dropout2)\n",
    "# model = Model(inputs=[input_layer, lbp_input, hog_input, gabor_input], outputs=output_layer)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.summary()''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or try this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and extract features\n",
    "train_folder = 'C:/Users/HP/Desktop/face_emotion_github/Data/Fer2013/archive/train/'\n",
    "train_images, train_labels, train_features = load_images(train_folder)\n",
    "\n",
    "# Load testing data and extract features\n",
    "test_folder = 'C:/Users/HP/Desktop/face_emotion_github/Data/Fer2013/archive/test/'\n",
    "test_images, test_labels, test_features = load_images(test_folder)\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_layer = Input(shape=(48, 48, 3))\n",
    "x = Conv2D(32, kernel_size=(3,3), activation='relu')(input_layer)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(64, kernel_size=(3,3), activation='relu')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(128, kernel_size=(3,3), activation='relu')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "lbp_input_layer = Input(shape=(26,))\n",
    "hog_input_layer = Input(shape=(3780,))\n",
    "gabor_input_layer = Input(shape=(32,))\n",
    "\n",
    "concat_layer = concatenate([x, lbp_input_layer, hog_input_layer, gabor_input_layer])\n",
    "hidden_layer = Dense(512, activation='relu')(concat_layer)\n",
    "output_layer = Dense(7, activation='softmax')(hidden_layer)\n",
    "\n",
    "model = Model(inputs=[input_layer, lbp_input_layer, hog_input_layer, gabor_input_layer], outputs=output_layer)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
